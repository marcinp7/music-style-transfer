% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[en]{pracamgr}

% Dane magistranta:
\autor{Marcin Papierzyński}{345782}

\title{Music Style Transfer}
\titlepl{Transfer Stylu Muzycznego}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{dr hab. Marek Cygan\\
  Instytut Informatyki
  }

% miesiąc i~rok:
\date{September 2019}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{11.4 Artificial Intelligence}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{Applied computing\\
  Arts and humanities\\
  Sound and music computing}

% Słowa kluczowe:
\keywords{style transfer, music, midi, neural networks}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  TODO
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter{Introduction}

Transfer of style between images is a well-known problem.
Currently it can be realized between any two images by using convolutional neural networks.
The analogous style transfer done for music is a much less explored topic and it's challenging even to define what exactly a musical style is.

In this paper the goal is to define some sensible notion of musical style and create a model that can create a different arrangement for any song based on style extracted from any other song.
The music used for training is assumed to be in MIDI format and can use any range of instruments.
The instruments supported by the model must be set during training, but the supported styles do not -- we want the trained model to be able to extract style from any song.

\chapter{Other works}

There exist previous works doing music style transfer to some extent.
A few examples are shortly described below.
All of them, however, are performing a simpler kind of style transfer.

\section{Neural Translation of Musical Style}

In Neural Translation of Musical Style \cite{neural_translation} authors created a model than can learn to perform any given piano piece in either jazz or classical style.
The model does this by regression of notes' velocities (loudness), so it's effectively creating an interpretation of a given piano piece.
Therefore it's much simpler, because it only supports one instrument (piano) and cannot create a completely different arrangement.

\section{Symbolic Music Genre Transfer with CycleGAN}

In Symbolic Music Genre Transfer with CycleGAN \cite{cyclegan} the described model can create new arrangements but it can still only generate piano and the styles need to be set during training (in this case it's jazz, classic and pop).

\section{Play as You Like: Timbre-enhanced Multi-modal Music Style Transfer}

In Timbre-enhanced Multi-modal Music Style Transfer \cite{multimodal} the model can operate on different instruments, but the styles still need to be set during training (the styles are actually instruments, the authors are using guitar, piano and string quartet).
Like in the previous examples, it doesn't allow to transfer style between any two songs.

\chapter{Overview and results}

\section{Musical style and composition}

For musical style transfer, the two main notions concerning songs are composition and style.
I start by giving a rough definition of them.

Composition is any information about a song that is related to specific moment in time, e.g. if the music gets louder or some instrument starts playing.
The most important part of composition is the melody (played by one or more instruments).

The style, on the other hand, is information not related to any specific moment in time, e.g. the general mood of the song or the way specific instruments are used (for example, which instruments play the main melodic line and which ones create a backing).

Since music is a form of art, even though it can be understood and formally described, it will always be at least partly subjective.
This means that the exact way to split songs into composition and style is not unique (for example, used instruments may naturally be considered part of style, but they can also change in time).
The main assumption, however, is that composition-style splitting can be performed in \emph{some} fashion which can be learned by a complex enough model.

Since any song can be naturally interpreted as a time sequence, I will use a recurrent neural network as a model for splitting the input song into composition and style.
Composition, just like the input, is a time series, but simpler, with less features.
The style, on the other hand, is a single vector representing the whole song.
During training, the model splits the input song into composition and style, and then combines them to recreate the original song.
Because composition is much smaller than the input song, the model must include useful information in the style vector as well.

After training, I can use the model to extract style from some song and combine it with the composition extracted from other song.
The hypothesis is that this approach will allow for a sensible music style transfer.

\section{Results}

TODO

\chapter{Framework}

\section{Tools}

The whole project is implemented in Python.
The machine learning framework I use is PyTorch, along with mido -- a Python library for working with MIDI files.

\section{Data}

The dataset I use is Lakh MIDI Dataset\footnote{https://colinraffel.com/projects/lmd/}.
It contains over 100,000 songs in MIDI format and covers various genres, including pop, rock or classical.

\chapter{Data flow}

The data flow of the whole architecture is comprised of 3 stages: style extraction, predicting song information, and style applying.

\section{Style extraction}

The input in the first stage is:
\begin{itemize}
\item instruments used in the song
\item tempo
\item mode of the song (major or minor)
\item song content (information about which notes are played in any moment in time for any instrument used in the song)
\end{itemize}

The output is comprised of 3 parts: melody, rhythm and style.
I will refer to melody and rhythm as composition.

Melody should encode song content (like the input) but with no information about specific instruments (it must combine them).
It encodes all pitched instruments used in the song.

Rhythm contains additional information about the melody (but not explicitly related to specific notes) and features of percussion if it is present in the input song.
The main reason for introducing rhythm alongside melody is to be able to represent percussion (the only unpitched instrument) but at the same time do not force it if it's not present in the original song (the model should be able to add percussion to a song that originally didn't have it).

I require that there is always at least one pitched instrument used in the song, while the percussion is optional.
That way both melody and rhythm are always well-definied (there is at least one pitched instrument they can encode).

Melody and rhythm, along with the style, should enable to recreate all the instruments used in the song.

\section{Predicting song info}

The input in this stage is style and rhythm.
The output is basic information about a song (as in the first stage):
\begin{itemize}
\item used instruments (classification)
\item tempo (regression)
\item mode (classification)
\end{itemize}

All those outputs should coincide with the respective inputs from the first stage.

\section{Style applying}

At this stage the input are composition and style from the first stage.
The output is the song content for each of the used instruments.
Generated song content should conincide with the input from the first stage.
In other words, the model must reconstruct the original song.

\chapter{Data representation}

In MIDI format, the song is encoded as a sequence of events messages.
Messages can contain instructions like setting the tempo or playing a note.
Note events are assigned to one of the 16 channels -- each of those channels can represent a different instrument.

\section{MIDI encoding}

The MIDI file is encoded as a tensor in the format:
\begin{center}
(\emph{batch, channel, bar, beat, beat fraction, note, note features}).
\end{center}

The \emph{channel} dimension refers to MIDI channel (an instrument).
The \emph{bar}, \emph{beat} and \emph{beat fraction} dimensions point to the exact moment in the song when the note is being played.

\subsection{Specifying time}

I assume that the time signature cannot change, so that the number of the beats is constant throughout the song (usually 3 or 4).
The \emph{bar} and \emph{beat} dimensions denote in which bar and at which beat the note should play.
The \emph{beat fraction} dimension is specifying the exact moment during the beat.

I divide the beat into 8 parts, which gives 8 possible fractions: 0, 1/8, 2/8, ..., 7/8.
Independently, I also divide the beat into 3 parts: 0, 1/3, 2/3.
Combining it, symplifying and sorting, we get 10 different fractions:
$$
0, \frac{1}{8}, \frac{1}{4}, \frac{1}{3}, \frac{3}{8}, \frac{1}{2}, \frac{5}{8}, \frac{3}{4}, \frac{2}{3}, \frac{7}{8}.
$$

Each \emph{note fraction} coordinate represents one of those fractions.
Including fractions with denominator 3 allows for the exact representation of triplets, which otherwise would need to be approximated using fractions with denominator 8.
It's worth noting that because of that, those fractions are not equidistant, meaning that it's not justified to apply convolution along \emph{note fraction} dimension.
However, the length of this dimension is not substantial, so it's not a big problem. More fractions could be added to allow for representing faster melodies, but these 10 are enough for most songs.

\subsection{Note features}

The above format is used for both percussion and pitched instruments.
However, the number of notes and note features (lengths of dimensions \emph{note} and \emph{note features} respectively) is different for percussion than for pitched instruments, so 2 tensors must be used: one for pitched instruments and one (possibly empty) for percussion.

Note features for pitched instruments are:
\begin{itemize}
\item velocity (loudness)
\item duration
\item accidentals (raise or lower the note one semitone)
\end{itemize}

For percussion the only note features are velocity and duration.

\section{Notes' representation}

I use note's span of 8 octaves, each with 12 sounds, which gives 96 possible notes in total.
Typical way of encoding notes is to have each coordinate in a \emph{note} dimension correspond to a specific note (so \emph{note} dimension would have length 96).
However, this way of encoding would pose a problem for style transfer.
Namely, depending on the mode of the song (major or minor), a different set of notes is used.
This means that the same song in a different mode would use different sounds in its melody.
Hence, with the typical way of encoding notes, the melody representation would partially impose style, which is not desired, since we want to split them.

To remedy that, I encode the notes relative to the scale the song is in.
Effectively, what is encoded are not the actual notes, but rather the scale degrees they represent; so the note C in a song in C major would be encoded the same way as the note G in a song in G minor.
That way changing the scale of the song will not affect the melody representation.

This way of encoding only allows to encode notes contained in a given scale (which covers 7 out of 12 notes in each octave).
For example, if the song is in C major, we could only encode "white keys".
To allow for encoding all the remaining notes as well, each note has additional features (called accidentals) that can raise it or lower it one semitone.
Then we can represent all 12 notes in each octave.

The downside of this solution is that it requires the scale of the song to be known -- and finding it is in itself not a trivial problem.
I use fairly simple heuristics based on the Krumhansl-Schmuckler key-finding algorithm\footnote{http://rnhart.net/articles/key-finding/}, which predicts the scale of the song based on the frequency of notes used.

\section{Style and composition representation}

Melody is a tensor of shape
\begin{center}
(\emph{batch, channel, bar, beat, beat fraction, note, features}),
\end{center}
which is the same as input shape but with no \emph{instrument} dimension.
The number of note features may also be different.

Rhythm is a tensor of shape
\begin{center}
(\emph{batch, channel, bar, beat, beat fraction, features}).
\end{center}
It's also similar to the input but this time with no \emph{instrument} and no \emph{note} dimensions.

Style is encoded as a regular vector, so it has shape
\begin{center}
(\emph{batch, features}).
\end{center}

Number of features in the melody should be low enough so that the model cannot simply remember all the instruments in the input.
Instead, the model will need to learn some compressed high-level representation of the melody, from which it will later be able to reconstruct the input instruments, using the style vector.

\chapter{Model}

TODO

Convolutions, LSTM.

The loss function is a combination of various loss functions:
\begin{itemize}
\item notes loss (smooth F1 score measuring if the model is playing the right notes)
\item accidentals, instruments and mode prediction loss (cross entropy)
\item velocity, duration and tempo regression loss (MSE)
\end{itemize}

\chapter{Experiments}

TODO

The model in each iteration is given one random song from the dataset.
It needs to split it into composition and style, and then reconstruct the original song.

After training, the model is used to extract style and composition from two songs and then generate songs from these compositions but with switched styles.

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography}

\bibitem{neural_translation} Iman Malik, Carl Henrik Ek, \textit{Neural Translation of Musical Style}, https://arxiv.org/abs/1708.03535.

\bibitem{cyclegan} Gino Brunner, Yuyi Wang, Roger Wattenhofer, Sumu Zhao, \textit{Symbolic Music Genre Transfer with CycleGAN}, https://arxiv.org/abs/1809.07575.

\bibitem{multimodal} Chien-Yu Lu, Min-Xin Xue, Chia-Che Chang, Che-Rung Lee, Li Su, \textit{Play as You Like: Timbre-enhanced Multi-modal Music Style Transfer}, https://arxiv.org/abs/1811.12214.

\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
